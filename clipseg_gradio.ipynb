{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bdd94b8-2d57-487e-bf01-03859f6d22e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "\n",
    "from transformers import CLIPSegProcessor, CLIPSegForImageSegmentation\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a582e1a-bf4f-45b4-971d-c929157d6f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cpu\")\n",
    "processor = CLIPSegProcessor.from_pretrained(\"CIDAS/clipseg-rd64-refined\")\n",
    "model = CLIPSegForImageSegmentation.from_pretrained(\"CIDAS/clipseg-rd64-refined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dee9ea96-a8ca-4a1f-ab48-6ca1c7fa34aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_prediction(pred, image, threhsold):\n",
    "    mask = Image.fromarray(np.uint8(pred * 255), \"L\")\n",
    "    mask = mask.convert(\"RGB\")\n",
    "    mask = mask.resize(image.size)\n",
    "    mask = np.array(mask)[:, :, 0]\n",
    "\n",
    "    # normalize the mask\n",
    "    mask_min = mask.min()\n",
    "    mask_max = mask.max()\n",
    "    mask = (mask - mask_min) / (mask_max - mask_min)\n",
    "\n",
    "    # threshold the mask\n",
    "    bmask = mask > threhsold\n",
    "    # zero out values below the threshold\n",
    "    mask[mask < threhsold] = 0\n",
    "\n",
    "    return bmask, mask\n",
    "\n",
    "def plot_heatmap(image, bmask, mask, alpha_value=0.5):\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.imshow(image)\n",
    "    ax.imshow(mask, alpha=alpha_value, cmap=\"jet\")\n",
    "\n",
    "    contours, hierarchy = cv2.findContours(\n",
    "        bmask.astype(np.uint8), cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE\n",
    "    )\n",
    "    for contour in contours:\n",
    "        x, y, w, h = cv2.boundingRect(contour)\n",
    "        rect = plt.Rectangle(\n",
    "            (x, y), w, h, fill=False, edgecolor=\"yellow\", linewidth=2\n",
    "        )\n",
    "        ax.add_patch(rect)\n",
    "\n",
    "    ax.axis(\"off\")\n",
    "    plt.tight_layout()\n",
    "\n",
    "    return fig\n",
    "\n",
    "def process_image(image, text_prompt, threhsold, visual_prompt=None):\n",
    "    inputs = processor(\n",
    "        text=text_prompt, images=image, padding=\"max_length\", return_tensors=\"pt\"\n",
    "    )\n",
    "\n",
    "    # predict\n",
    "    with torch.no_grad():\n",
    "        if visual_prompt is not None:\n",
    "            encoded_prompt = processor(images=visual_prompt, return_tensors=\"pt\")\n",
    "            outputs = model(**inputs, conditional_pixel_values=encoded_prompt.pixel_values)\n",
    "        else:\n",
    "            outputs = model(**inputs)\n",
    "\n",
    "        preds = outputs.logits\n",
    "\n",
    "    pred = torch.sigmoid(preds)\n",
    "    bmask, mask = process_prediction(pred.cpu().numpy(), image, threhsold)\n",
    "    segmented_image = (np.array(image) * mask[:, :, None]).astype(int)\n",
    "    heatmap = plot_heatmap(image, bmask, mask)\n",
    "\n",
    "    return heatmap, mask, segmented_image\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    title = \"Interactive demo: zero-shot image segmentation with CLIPSeg\"\n",
    "    description = \"Demo for using CLIPSeg, a CLIP-based model for zero- and one-shot image segmentation. To use it, simply upload an image and add a text to mask (identify in the image), or use one of the examples below and click 'submit'. Results will show up in a few seconds.\"\n",
    "    article = \"<p style='text-align: center'><a href='https://arxiv.org/abs/2112.10003'>CLIPSeg: Image Segmentation Using Text and Image Prompts</a> | <a href='https://huggingface.co/docs/transformers/main/en/model_doc/clipseg'>HuggingFace docs</a></p>\"\n",
    "\n",
    "\n",
    "    with gr.Blocks() as demo:\n",
    "        gr.Markdown(\"# CLIPSeg: Image Segmentation Using Text and Image Prompts\")\n",
    "        gr.Markdown(article)\n",
    "        gr.Markdown(description)\n",
    "\n",
    "        with gr.Row():\n",
    "            with gr.Column():\n",
    "                input_image = gr.Image(type=\"pil\")\n",
    "                input_prompt = gr.Textbox(label=\"Please describe what you want to identify\")\n",
    "                input_slider_T = gr.Slider(\n",
    "                    minimum=0, maximum=1, value=0.4, label=\"Threshold\"\n",
    "                )\n",
    "                btn_process = gr.Button(label=\"Process\")\n",
    "\n",
    "            with gr.Column():\n",
    "                output_plot = gr.Plot(label=\"Segmentation Result\")\n",
    "                output_mask = gr.Image(label=\"Mask\")\n",
    "                output_image = gr.Image(label=\"Segmented Image\")\n",
    "\n",
    "        btn_process.click(\n",
    "            process_image,\n",
    "            inputs=[\n",
    "                input_image,\n",
    "                input_prompt,\n",
    "                input_slider_T\n",
    "            ],\n",
    "            outputs=[output_plot, output_mask, output_image],\n",
    "        )\n",
    "\n",
    "    demo.launch(share=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f9927a0-bcf8-4351-a283-bacdd95d83b0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
